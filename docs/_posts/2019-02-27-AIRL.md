---
title: "AIRL â€” Adversarial Inverse Reinforcement Learning"
excerpt: "We introduce a practical GAN-style IRL algorithm named adversarial inverse reinforcement learning(AIRL)"
categories:
  - Reinforcement Learning
tags:
  - Inverse RL
---

##  Introduction

Part of the challenge of IRL is that IRL is an ill-defined problem, since there are 1) many optimal policies that can explain the demonstrations, and many rewards that can explain the optimal policy. MaxEnt IRL discussed in our previous posts handles the former ambiguity, but the later ambiguity is still unaddressed, meaning that IRL algorithms have difficulty distinguishing the true reward functions from those shaped by the environment dynamics. While shaped rewards can increase learning speed in the original training environment, when the reward is deployed at test-time on environments with varying dynamics, it may no longer produce optimal behaviors. In this post, we introduce adversarial inverse reinforcement learning (AIRL) that attempts to address this issue.

## Preliminaries

In our previous post, we cast the MaxEnt IRL problem into a GAN optimization problem, and derive the optimal discriminator as

$$
\begin{align}
D(\tau)={\exp(f(\tau))\over\exp(f(\tau))+\pi(\tau)}
\end{align}
$$

where $$f$$ is the optimal reward function up to some constants as reward shaping does not affect the optimization problem. Note that we've folded $$Z$$ into $$f$$ as there is no way to uniquely learn $$Z$$(see [this answer](https://openreview.net/forum?id=rkHywl-A-&noteId=S1Nj--xSG)). 

Learning from trajectories could noisy due to the high variance. Therefore, we instead convert it into the single state and action case

$$
\begin{align}
D(s,a)={\exp(f(s,a))\over\exp(f(s,a))+\pi(a|s)}\tag{1}\label{eq:1}
\end{align}
$$

We can show that, at optimality, $$f^*(s,a)=\log\pi^*(a\vert s)=A^*(s,a)$$ the advantage function of the optimal policy.

## The Reward Ambiguity Problem

Equation $$\eqref{eq:1}$$ does not put any form of requirements on the reward function. As a result, there is no guarantee in $$f(s,a)$$ to be the real reward function; $$f$$ is free to be any shaped reward function of form since reward shaping preserves the optimal policy. Mathematically, $$f$$ might learn to be any reward function of the following form

$$
\begin{align}
f(s,a,s')=r(s,a,s')+\gamma\Phi(s')-\Phi(s)
\end{align}
$$

where $$\Phi:\mathcal S\rightarrow\mathbb R$$ can be any function. Consequently, the reward function may not robust to changes in dynamics. Consider deterministic dynamics $$\mathcal T(s,a)\rightarrow s'$$ and state-action rewards $$\hat r(s,a)=r(s,a)+\gamma\Phi(\mathcal T(s,a))-\Phi(s)$$. It is easy to see that changing the dynamics $$\mathcal T$$ to $$\mathcal T'$$ such that $$\mathcal T'(s,a)\ne\mathcal T(s,a)$$ means that $$\hat r(s,a)$$ is no longer shaped in the same way as before.

##  Adversarial Inverse Reinforcement Learning

To address the reward ambiguity problem, AIRL employs an additional shaping term to mitigate the effects of unwanted shaping.

Formally, AIRL defines $$f$$ as

$$
\begin{align}
f_{\psi,\phi}(s,a,s')=g_\psi(s)+\gamma h_\phi(s')-h_\phi(s)
\end{align}
$$

where, ideally, $$g_\psi$$ is optimized to be the ground truth reward function of the state plus some constant, $$h_\phi$$ is optimized to be the optimal state value function plus some constant. As a shaping function, $$h_\phi$$ helps mitigate the effects of unwanted shaping on the reward approximator $$g_\psi$$.

### Algorithm


$$
\begin{align}
&\mathbf{AIRL:}\\\
&\quad \text{Initialize policy }\pi_\theta \text{ and discriminator } D_{\psi,\phi}\\\
&\quad \mathbf{For}\ i=1\mathrm{\ to\ }N:\\\
&\quad\quad \text{Generate samples }\mathcal D_{samp}\ \text{from }\pi_\theta\\\
&\quad\quad \text{Sample expert demonstration}\ \mathcal D_{demo}\\\
&\quad\quad \text{Update }\psi\ \mathrm{and}\ \phi\ \text{in discriminator}\ D_{\psi,\phi}\text{ using }\mathcal D_{samp}\text{ and }\mathcal D_{demo}\\\
&\quad\quad \text{Update reward}\\\
&\quad\quad \text{Update }\theta\text{ in policy }\pi_\theta\mathrm{ using }\mathcal D_{samp}
\end{align}
$$


This algorithm bears much resemblance to GAN-GCL, with two difference: 1. we define the discriminator as $$D_{\psi,\phi}$$ in Eq.$$\eqref{eq:8}$$, and 2. we update the discriminator using transitions instead of trajectories.

## References

<a name="ref1"></a>Justin Fu, Katie Luo, Sergey Levine. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning

## Supplementary Materials

