---
title: "Ensemble"
excerpt: "In which we discuss three commonly used Ensemble techniques"
categories:
  - Machine Learning
tags:
  - Machine Learning
---

Ensemble methods have been widely used in data science competitions to combine the strength of different models and whereby achieving better final performance. We concisely discuss three types of ensemble methods here:

1. Bagging(Bootstrap Aggregating) independently trains multiple models on different subsets of data(usually sampled with replacement) and combines the results of these models via some averaging process for prediction. It is an effective method when you have limited data in that it eases the overfitting problem thanks to the random sampling and averaging process.
2. Boosting sequentially builds an ensemble by training each new model to correct the mistakes of its predecessors. More specifically, boosting repeats the following steps: 1) Fit a new model to the residual from the current ensemble; 2) add the new model(usually rescaled by some learning rate) to the ensemble to make predictions; 3) recompute the residual for each training sample. In practice, we primarily apply boosting to reduce bias. Therefore, one might well assume models adopted by boosting have high bias but low variance.
3. Stacking can be summarized in a two-level framework: In the first level, it trains a variety of base models on the original training set; in the second level, it constructs a new dataset by grouping the predictions of the base models as new features, and trains a meta-learner on this new dataset. Both base models and the meta-learner learn to predict the same target. Specifically, when the meta-learner is a linear model, we can regard the stacked model as an intelligent bagging model, where the meta-learner learns to combine and weight the results of base models instead of naively averaging the results.


