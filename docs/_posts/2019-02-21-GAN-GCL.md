---
title: "GAN-GCL"
excerpt: "We build a connection between maximum entropy inverse reinforcement learning and generative adversarial networks"
categories:
  - Reinforcement Learning
tags:
  - Inverse RL
---

##  Introduction

In the previous post, we talked about maximum entropy inverse reinforcement learning(MaxEnt), and introduce a practical sample-based algorithm named guided cost learning(GCL) that allows us to tackle high-dimensional state and action spaces and nonlinear reward functions. In GCL, we saw that the reward function learning in fact competed against policy learning, which makes them resemble GANs. In this post, we will build a connection between MaxEnt IRL and GANs, and introduce an algorithm called generalized adversarial network guided cost learning (GAN-GCL), which employs GANs' training process to theoretically achieving the same behavior as guided cost learning(GCL). 

## Preliminaries

### Generative Adversarial Networks

In GANs, there are two models trained simultaneously: a generator $$G$$ and a discriminator $$D$$. The discriminator is responsible for classifying its inputs as either the output of the generator or real samples from the underlying distribution $$p(x)$$, which is generally unknown. The generator, on the other hand, aims to produce outputs that are real enough to "fool" the discriminator.

Formally, we have the loss for the discriminator

$$
\begin{align}
\mathcal L_{D}(\psi)=\mathbb E_{x\sim p}[-\log D(x)]+\mathbb E_{x\sim G}[-\log(1-D(x))]\tag{1}\label{eq:1}
\end{align}
$$

and the loss for the generator

$$
\begin{align}
\mathcal L_G(\theta)=\mathbb E_{x\sim G}[-\log D(x)]+\mathbb E_{x\sim G}[\log (1-D(x))]\tag{2}\label{eq:2}
\end{align}
$$

where the second term is the opposite of the discriminator's loss, and the first term is the log of the discriminator confusion, which is introduced as an improvement in the initial paper of GANs.

### Maximum Entropy Inverse Reinforcement Learning

In MaxEnt IRL, we have (near-)optimal trajectories $$\{\tau_i\}$$ sampled from some underlying trajectory policy distribution $$p(\tau)$$, which is generally unknown. We train a reward function so that the log likelihood of the demonstrations is maximized when the agent acts optimally according to the reward function

$$
\begin{align}
\mathcal J_r(\psi)=\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=\mathbb E_{\tau\sim p}[r_\psi(\tau)]-\log Z\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over {1\over 2}\tilde p(\tau)+{1\over 2}q_\theta(\tau)}\right]
\end{align}
$$

where $$Z$$ is the partition function estimated using the sampling distribution $$\mu={1\over 2}p+{1\over 2}q$$, $$\tilde p(\tau)$$ in the sampling weights is a rough estimate for the expert trajectory distribution $$p(\tau)$$, and $$q_\theta(\tau)$$ (which is denoted as $$\pi$$ in our previous post) is trained to be the soft optimal policy w.r.t. the current reward function by maximizing the objective

$$
\begin{align}
\mathcal J_q(\theta)=\mathbb E_{\tau\sim q}[r_\psi(\tau)+\mathcal H(q_\theta(\tau))]
\end{align}
$$

To align with GANs, we negate the objective so that we can perform minimization as we do in GANs. Therefore, we have

$$
\begin{align}
\mathcal L_r(\psi)=-\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=-\mathbb E_{\tau\sim p}[r_\psi(\tau)]+\log Z\tag{3}\label{eq:3}\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over {1\over 2}\tilde p(\tau)+{1\over 2}q_\theta(\tau)}\right]\\\
\mathcal L_q(\theta)&=\mathbb E_{\tau\sim q}[-r_\psi(\tau)+\mathcal \log(q_\theta(\tau))]\tag{4}\label{eq:4}
\end{align}
$$



## Connection between MaxEnt IRL and GANs

For a fixed generator with density $$q(\tau)$$, the optimal discriminator is

$$
\begin{align}
D^*(\tau)={p(\tau)\over p(\tau)+q(\tau)}
\end{align}
$$

where $$p(\tau)$$ is the distribution of the real data.

When $$q(\tau)$$ could be evaluated, the traditional GAN discriminator can be modified to incorporate this density information. Instead of having the discriminator estimate the probability that its input is real, we can use the discriminator to estimate the distribution of the real data $$p(\tau)$$, filling in the value of $$q(\tau)$$ with its known value. In that case, the new form of the discriminator $$D_\psi$$ is 

$$
\begin{align}
D_\psi(\tau)={\tilde p_\psi(\tau)\over\tilde p_\psi(\tau)+q_\theta(\tau)}\tag{5}\label{eq:5}
\end{align}
$$

This modest change allows the optimal discriminator to be completely independent of the generator: the discriminator is optimal when $$\tilde p_\psi(\tau)=p(\tau)$$. Independence between the generator and the optimal discriminator may significantly improve the stability of training.[2]

In order to build a connection to MaxEnt IRL, we replace $$\tilde p_\psi(\tau)$$ with $${1\over Z}\exp(r_\psi(\tau))$$—recall that we establish $$p(\tau\vert O_{1:T})={1\over Z}\exp(r_\psi(\tau))$$ in our [previous post]({{ site.baseurl }}{% post_url 2019-02-14-GCL %})—and have

$$
\begin{align}
D_\psi(\tau)&={ {1\over Z}\exp(r_\psi(\tau))\over {1\over Z}\exp(r_\psi(\tau))+q_\theta(\tau)}\tag{6}\label{eq:6}\\\
&={\exp\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)\over\exp\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)+1}\\\
&=\sigma\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)
\end{align}
$$

This gives us a nice architecture of the discriminator: a simple binary classification with a sigmoid as the final layer. The only adjustment here is to subtract $$\log Z$$ and $$\log q_\theta(\tau)$$ from the input to the sigmoid.

Now we stick the discriminator in Eq.$$\eqref{eq:6}$$ back into the discriminator loss defined in Eq.$$\eqref{eq:1}$$, which results in the following loss function:

$$
\begin{align}
\mathcal L_{D}(\psi)&=\mathbb E_{\tau\sim p}[-\log D_\psi(\tau)]+\mathbb E_{\tau\sim q}[-\log(1-D_\psi(\tau))]\\\
&=\mathbb E_{\tau\sim p}\left[-\log { {1\over Z}\exp(r_\psi(\tau))\over 2\tilde\mu(\tau)}\right]+\mathbb E_{\tau\sim q}\left[-\log{q_\theta(\tau)\over 2\tilde\mu(\tau)}\right]\tag{7}\label{eq:7}
\end{align}
$$

where we replace $${1\over 2Z}\exp(r_\psi(\tau))+{1\over 2}q_\theta(\tau)$$ with $$\tilde\mu(\tau)$$ to simplify notation. Similarly, we rewrite Eq.$$\eqref{eq:3}$$ as

$$
\begin{align}
\mathcal L_r(\psi)=\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=-\mathbb E_{\tau\sim p}[r_\psi(\tau)]+\log Z\tag{8}\label{eq:8}\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]
\end{align}
$$

Next, we will establish the following facts, which together imply that GANs optimize precisely the MaxEnt IRL problem:

1. The value of $$Z$$ that minimizes the discriminator's loss is an importance-sampling estimator for the partition function defined in Eq.$$\eqref{eq:8}$$.
2. For this value of $$Z$$, the derivative of the discriminator's loss w.r.t. $$\psi$$ is equal to the derivative of the MaxEnt IRL objective.
3. The generator's loss is exactly equal to the objective of the soft optimal policy defined in Eq.$$\eqref{eq:4}$$.

### $$Z$$ Estimates The Partition Function

We first show that minimize $$\mathcal L_D(\psi)$$ w.r.t. $$Z$$ in Eq.$$\eqref{eq:7}$$ gives us the exactly importance-sampling estimator for the partition function in Eq.$$\eqref{eq:8}$$:

$$
\begin{align}
\partial_Z\mathcal L_D(\psi)&=0\\\
\partial_Z\Big(\log Z+\mathbb E_{\tau\sim p}[\log\tilde\mu(\tau)]+\mathbb E_{\tau\sim q}[\log\tilde\mu(\tau)]\Big)&=0\\\
\partial_Z\Big(\log Z+2\mathbb E_{\tau\sim \mu}[\log\tilde\mu(\tau)]\Big)&=0\\\
{1\over Z}&=\mathbb E_{\tau\sim\mu}\left[{ {1\over Z^2}\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]\\\
Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]
\end{align}
$$

where in the second step we omit terms irrelevant to $$Z$$, and notice $$\tilde \mu(\tau)={1\over 2Z}\exp(r_\psi(\tau))+{1\over 2}q_\theta(\tau)$$. This nice result suggests that now we are no longer need to estimate the importance weights anymore. Instead, we can directly estimate the partition function by minimizing the discriminator's loss w.r.t. $$Z$$. Furthermore, as we will see shortly, this also plays a critical role in the following proofs. 

### $$r_\psi$$ optimizes the MaxEnt IRL objective

Consider the derivative of the discriminator's loss (Eq.$$\eqref{eq:7}$$) w.r.t. parameters $$\psi$$

$$
\begin{align}
\partial_\psi\mathcal L_D(\psi)&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+\partial_\psi\big(\mathbb E_{\tau\sim p}[\log\tilde\mu(\tau)]+\mathbb E_{\tau\sim q}[\log\tilde\mu(\tau)]\big)\\\
&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+2\partial_\psi\mathbb E_{\tau\sim \mu}[\log\tilde\mu(\tau)]\\\
&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+\mathbb E_{\tau\sim\mu}\left[{ {1\over Z}\exp(r_\psi(\tau))\partial_\psi r_\psi(\tau)\over\tilde\mu(\tau)}\right]\\\
&=-\mathbb E_{\tau\sim p}[\partial_\psi r_\psi(\tau)]+{1\over Z}\mathbb E_{\tau\sim\mu}\left[{\exp(r_\psi(\tau))\partial_\psi r_\psi(\tau)\over\tilde\mu(\tau)}\right]\\\
&=\partial_\psi\mathcal L_r(\psi)
\end{align}
$$

where in the first step we omit terms irrelevant to $$\psi$$. The last equality holds because $$Z$$ in the discriminator estimates the partition function in the MaxEnt IRL objective and we do not differential through the importance weights when we compute the derivative of the MaxEnt IRL objective. This result suggests that applying gradient descent to the discriminator in GAN is equal to doing the same to the MaxEnt IRL objective.

### The Generator Optimizes The Soft Optimal Policy

Remember that we assume the generator density to be $$q_\theta(\tau)$$. Now we compute the generator's loss defined in Eq.$$\eqref{eq:2}$$:

$$
\begin{align}
\mathcal L_{G}(\theta)&=\mathbb E_{\tau\sim q}[-\log D(\tau)+\log(1-D(\tau))]\\\
&=\mathbb E_{\tau\sim q}\left[-\log{ {1\over Z}\exp(r_\psi(\tau))\over 2\tilde \mu}+\log{q_\theta(\tau)\over 2\tilde\mu}\right]\\\
&=\log Z+\mathbb E_{\tau\sim q}\left[\log q_\theta(\tau)-r_\psi(\tau)\right]\\\
&=\log Z+\mathcal L_q(\theta)
\end{align}
$$

Since $$Z$$ is trained with the discriminator that is fixed while optimizing the generator, this loss is exactly equivalent to the policy loss Eq.$$\eqref{eq:4}$$ from MaxEnt IRL.

### Summary

With all the equipment at our disposal, now let us formulate the algorithm

$$
\begin{align}
&\mathbf{GAN\_GCL:}\\\
&\quad \mathrm{Initialize\ policy\ }\pi_\theta\ \mathrm{and\ discriminator\ }\mathcal D_{\psi}\\\
&\quad \mathbf{For}\ i=1\mathrm{\ to\ }N:\\\
&\quad\quad \mathrm{Generate\ samples\ }\mathcal D_{samp}\ \mathrm{from\ }\pi_\theta\\\
&\quad\quad \mathrm{Sample\ expert\ demonstration}\ \mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ }\psi\ \mathrm{in\ discriminator}\ D_\psi\mathrm{\ using\ }\mathcal D_{samp}\mathrm{\ and\ }\mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ \theta\ in\ policy\ }\pi_\theta\mathrm{\ using\ }\mathcal D_{samp}
\end{align}
$$


As the authors of AIRL answered [here](https://openreview.net/forum?id=rkHywl-A-&noteId=S1Nj--xSG): in practice, $$Z$$ cannot be learned uniquely, and should be considered as a part of the discriminator.

## References

1. CS 294-112 at UC Berkeley. Deep Reinforcement Learning Lecture 16
2. <a name="ref2"></a>Chelsea Finn et al. A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models
3. Ian J. Goodfellow et al. Generative Adversarial Nets.
5. Andrew Ng et al. Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping

