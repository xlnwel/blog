---
title: "Lessons Learned from Procgen Competition"
excerpt: "Some lessons learned in retrospect"
categories:
  - Reinforcement Learning
tags:
  - Introspection
---

The following lessons are learned from the Procgen competition. Unfortunately, I realized most of them near the end of/after the competition :-(

2. Policy gradient methods such as PPO and PPG are more suitable for Procgen environments than Q-learning methods. My journey of Procgen competition seemed doomed to failure because of my choice of value-based algorithm. However, this was something inevitable as I didn't have a 16G GPU to run PPO on my local machine and more importantly, my algorithm performed well on the environments exposed in Round 1 and Round 2. In fact, I realized that policy gradient methods might be favored for this competition about two weeks before the completion of Round 2 because my algorithm got different performance on the local and remote machine. This is due to the nature of Ape-X, which requires a delicate balance between the training speed and data sampling speed. However, this phenomenon was never reported by other teams. But at that point, I didn't have the time and resources to tune PPO and I didn't even know PPG!
3. Value-based algorithms exhibits higher variance than policy based algorithm. Entropy bonus may help in some cases, but in general, it reduces the performance. The adversarial effect become more obvious when working with multi-step returns.
4. Reading papers helps, but do not expect that it will help you reach a good rank. I read dozens of papers after implementing a basic prototype. Now looking back, I find that few of them really helped. Techniques like auxiliary tasks and data augmentations advocated by many recent papers end up fruitlessâ€”in terms of sample efficiency. The failure of data augmentation makes sense since I haven't done any experiments on generalization. But it's quite surprising that most auxiliary tasks do not pay off (I've tried contrastive learning, deepMDP and some other auxiliary tasks). There are three main reasons I can conceive: 1) Auxiliary tasks such as contrastive learning may cause a distraction to the agent. Such a distraction may help the agent quickly understand the worldâ€”e.g. the CURL paper only run the agent for 100K time stepsâ€”but in the long term, they hurts. 2) Auxiliary tasks such as DeepMDP incurs additional overheads to the training stage. This affects the ratio of the number of training steps to the number of environment interaction because I use Ape-X for distributed training. 3) I fail to implement them right, or there are some details I missed or some hyperparameters I tuned wrongly.
5. Patience. I paid a big price for that. I learnt that the results of RL algorithms could be noisy, but I did not expect that they were that noisy. Here's my footprints: at the first, I relied on the result of a single incomplete trial to decide whether some modification is good. Soon I found some useful modification might not exhibit good performance in the first place, then I made judges based on a complete trial and a single submission. Until one day in the penultimate week, I mistakenly made two identical submissions, finding that the first was scored at $$0.623$$ but the second was $$0.509$$... Thereafter, I made submissions more carefully and usually made two identical submissions at a time. At the final week, I tended to rely on my belief to make judgement when I saw two methods of similar scores. 
6. Analyze the most fruitful parts of your method and prune the rest. Simplifying the code can help clear your thoughts and understand what's going on. After the competition, I analyze the agent and find the that most fruitful parts of my algorithm come from four aspects: 1) IQN and PER. 2) epsilon and temperature schedule 3) the balance between the training steps and environment interactions 4) the stochastic action and entropy bonus. During the competition, I used an additional actor to achieve stochastic behavior. After the analysis, I found that this actor was not necessary. The main gain of this actor is that it reduces the number of training steps and prevents the algorithms from overfitting. In light of that, I can achieve the same effect by removing the actor and devoting the resources to other places, e.g., multi-step learning.
7. Schedule your energy. The first thing done in the morning is important for the rest of the day. I constantly observe that if I start my day with easy task(e.g., writing code or monitoring experiments), I will not be able to do difficult tasks in that day(e.g., calming down to read a difficult paper).
8. Take a small step unless you are absolutely confident about the changes. This is especially important when tuning the hyperparameters. To me, confidence usually means I correct some obvious mistakes. 
9. Be sure your code works exactly as you expected it to be when you runs it. This usually means you have to write test code whenever you add or modify something nontrivial.
10. Logging experimental results is important, but it is more important to plan on (and record) what you're gonna to do. Well, I realize this when reading GTDâ€”one month after the competition completed.
11. It was my first time to write an algorithm in a framework. It really took me some time to get used to itâ€”not mention that ray0.8.5 does not support TensorFlow 2.x very well. There are two lessons I learned(also the 2 mistakes I madeðŸ˜’): 1) focus on what's matter to you and don't try to get every detail clear. Trying to get every detail you encounter clear can easily shift your attention away and end up wasting a lot of time. Again, keeping a note/mindmap helps. 2) it's okay to copy-paste and modify some library codeâ€”they're not set in stone!
12. Stop when you feel like hitting a dead end. Well, most of the above lessons are learned in retrospect. If I can do it earlier, maybe I won't waste that amount of time.
