---
title: "GAN-GCL, AIRL"
excerpt: "In which we build a connection between maximum entropy inverse reinforcement learning and generative adversarial networks, and introduce a practical GAN-style algorithm named adversarial inverse reinforcement learning(AIRL)"
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Inverse Reinforcement Learning
---

##  Introduction

In the previous post, we talked about maximum entropy inverse reinforcement learning(MaxEnt), and introduce a practical sample-based algorithm named guided cost learning(GCL) that allows us to tackle high-dimensional state and action spaces and nonlinear reward functions. In GCL, we saw that the reward function learning in fact competed against policy learning, which makes them resemble GANs. In this post, we will build a connection between MaxEnt IRL and GANs, and introduce an algorithm called generalized adversarial network guided cost learning (GAN-GCL), which employs GANs' training process to theoretically achieving the same behavior as guided cost learning(GCL). In the end, we will briefly introduce adversarial inverse reinforcement learning (AIRL), a practical and scalable IRL algorithm built upon GAN-GCL.

## Preliminaries

### Generative Adversarial Networks

In GANs, there are two models trained simultaneously: a generator $$G$$ and a discriminator $$D$$. The discriminator is responsible for classifying its inputs as either the output of the generator or real samples from the underlying distribution $$p(x)$$, which is generally unknown. The generator, on the other hand, aims to produce outputs that are real enough to "fool" the discriminator.

Formally, we have the loss for the discriminator

$$
\mathcal L_{D}(\psi)=\mathbb E_{x\sim p}[-\log D(x)]+\mathbb E_{x\sim G}[-\log(1-D(x))]\tag {1}
$$

and the loss for the generator

$$
\mathcal L_G(\theta)=\mathbb E_{x\sim G}[-\log D(x)]+\mathbb E_{x\sim G}[\log (1-D(x))]\tag{2}
$$

where the second term is the opposite of the discriminator's loss, and the first term is the log of the discriminator confusion, which is introduced as an improvement in the initial paper of GANs.

For more details and practical uses of GANs, please refer to my [previous post]({ { site.baseurl }}{% post_url 2018-08-16-GAN %}) or read up the papers[2] [3].

### Maximum Entropy Inverse Reinforcement Learning

In MaxEnt IRL, we have (near-)optimal trajectories $$\{\tau_i\}$$ sampled from some underlying trajectory policy distribution $$p(\tau)$$, which is generally unknown. We train a reward function so that the log likelihood of the demonstrations is maximized when the agent acts optimally according to the reward function

$$
\begin{align}
\mathcal L_r(\psi)=\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=\mathbb E_{\tau\sim p}[r_\psi(\tau)]-\log Z\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over {1\over 2}\tilde p(\tau)+{1\over 2}q_\theta(\tau)}\right]
\end{align}
$$

where $$Z$$ is the partition function estimated using the sampling distribution $$\mu={1\over 2}p+{1\over 2}q$$, $$\tilde p(\tau)$$ in the sampling weights is a rough estimate for the expert trajectory distribution $$p(\tau)$$, and $$q_\theta(\tau)$$ (which is denoted as $$\pi$$ in our previous post) is trained to be the soft optimal policy w.r.t. the current reward function by maximizing the objective

$$
\mathcal L_q(\theta)=\mathbb E_{\tau\sim q}[r_\psi(\tau)+\mathcal H(q_\theta(\tau))]
$$

To align with GANs, we negate the objective so that we can perform minimization as we do in GANs. Therefore, we have

$$
\begin{align}
\mathcal L_r(\psi)=\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=-\mathbb E_{\tau\sim p}[r_\psi(\tau)]+\log Z\tag {3}\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over {1\over 2}\tilde p(\tau)+{1\over 2}q_\theta(\tau)}\right]\\\
\mathcal L_q(\theta)&=\mathbb E_{\tau\sim q}[-r_\psi(\tau)+\mathcal \log(q_\theta(\tau))]\tag {4}
\end{align}
$$



## Connection between MaxEnt IRL and GANs

For a fixed generator with density $$q(\tau)$$, the optimal discriminator is

$$
D^*(\tau)={p(\tau)\over p(\tau)+q(\tau)}
$$

where $$p(\tau)$$ is the distribution of the real data.

When $$q(\tau)$$ could be evaluated, the traditional GAN discriminator can be modified to incorporate this density information. Instead of having the discriminator estimating the probability that its input is real, we can use the discriminator to estimate the distribution of the real data $$p(\tau)$$, filling in the value of $$q(\tau)$$ with its known value. In that case, the new form of the discriminator $$D_\psi$$ is 

$$
D_\psi(\tau)={\tilde p_\psi(\tau)\over\tilde p_\psi(\tau)+q_\theta(\tau)}\tag {5}
$$

This modest change allows the optimal discriminator to be completely independent of the generator: the discriminator is optimal when $$\tilde p_\psi(\tau)=p(\tau)$$. Independence between the generator and the optimal discriminator may significantly improve the stability of training.[2]

In order to build a connection to MaxEnt IRL, we replace $$\tilde p_\psi(\tau)$$ with $${1\over Z}\exp(r_\psi(\tau))$$ and have

$$
\begin{align}
D_\psi(\tau)&={ {1\over Z}\exp(r_\psi(\tau))\over {1\over Z}\exp(r_\psi(\tau))+q_\theta(\tau)}\tag {6}\\\
&={\exp\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)\over\exp\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)+1}\\\
&=\sigma\big(r_\psi(\tau)-\log Z-\log q_\theta(\tau)\big)
\end{align}
$$

This gives a nice architecture of the discriminator: a simple binary classification with a sigmoid as the final layer. The only adjustment here is to subtract $$\log Z$$ and $$\log q_\theta(\tau)$$ from the input to the sigmoid.

Now we stick the discriminator in Eq.$$(6)$$ back into the discriminator loss defined in Eq.$$(1)$$, which results in the following loss function:

$$
\begin{align}
\mathcal L_{D}(\psi)&=\mathbb E_{\tau\sim p}[-\log D_\psi(\tau)]+\mathbb E_{\tau\sim q}[-\log(1-D_\psi(\tau))]\\\
&=\mathbb E_{\tau\sim p}\left[-\log { {1\over Z}\exp(r_\psi(\tau))\over 2\tilde\mu(\tau)}\right]+\mathbb E_{\tau\sim q}\left[-\log{q_\theta(\tau)\over 2\tilde\mu(\tau)}\right]\tag {7}
\end{align}
$$

where we replace $${1\over 2Z}\exp(r_\psi(\tau))+{1\over 2}q_\theta(\tau)$$ with $$\tilde\mu(\tau)$$ to simplify notation. 

As we did in Eq.$$(6)$$, we also replace $$\tilde p_\psi(\tau)$$ with $${1\over Z}\exp(r_\psi(\tau))$$ for the log-likelihood objective of MaxEnt IRL defined in Eq.$$(3)$$:

$$
\begin{align}
\mathcal L_r(\psi)=\mathbb E_{\tau\sim p}[\log p_\psi(\tau)]&=-\mathbb E_{\tau\sim p}[r_\psi(\tau)]+\log Z\tag {8}\\\
\mathrm{where}\quad Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]
\end{align}
$$

Next, we will estabilish the following facts, which together imply that GANs optimize precisely the MaxEnt IRL problem:

1. The value of $$Z$$ that minimizes the discriminator's loss is an importance-sampling estimator for the partition function defined in Eq.$$(8)$$.
2. For this value of $$Z$$, the derivative of the discriminator's loss w.r.t. $$\psi$$ is equal to the derivative of the MaxEnt IRL objective.
3. The generator's loss is exactly equal to the objective of the soft optimal policy defined in Eq.$$(4)$$.

### $$Z$$ Estimates The Partition Function

We first show that minimize $$\mathcal L_D(\psi)$$ w.r.t. $$Z$$ in Eq.$$(7)$$ gives us the exactly importance-sampling estimator for the partition function in Eq.$$(8)$$:

$$
\begin{align}
\partial_Z\mathcal L_D(\psi)&=0\\\
\partial_Z\Big(\log Z+\mathbb E_{\tau\sim p}[\log\tilde\mu(\tau)]+\mathbb E_{\tau\sim q}[\log\tilde\mu(\tau)]\Big)&=0\\\
\partial_Z\Big(\log Z+2\mathbb E_{\tau\sim \mu}[\log\tilde\mu(\tau)]\Big)&=0\\\
{1\over Z}&=\mathbb E_{\tau\sim\mu}\left[{ {1\over Z^2}\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]\\\
Z&=\mathbb E_{\tau\sim \mu}\left[{\exp(r_\psi(\tau))\over \tilde \mu(\tau)}\right]
\end{align}
$$

where in the second step we omit terms irrelevant to $$Z$$, and notice $$\tilde \mu(\tau)={1\over 2Z}\exp(r_\psi(\tau))+{1\over 2}q_\theta(\tau)$$. This nice result suggests that now we are no longer need to estimate the importance weights anymore. Instead, we can directly estimate the partition function by minimizing the discriminator's loss w.r.t. $$Z$$. Furthermore, as we will see shortly, this also plays a critical role in the following proofs. 

### $$r_\psi$$ optimizes the MaxEnt IRL objective

Consider the derivative of the discriminator's loss (Eq.$$(8)$$) w.r.t. parameters $$\psi$$

$$
\begin{align}
\partial_\psi\mathcal L_D(\psi)&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+\partial_\psi\big(\mathbb E_{\tau\sim p}[\log\tilde\mu(\tau)]+\mathbb E_{\tau\sim q}[\log\tilde\mu(\tau)]\big)\\\
&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+2\partial_\psi\mathbb E_{\tau\sim \mu}[\log\tilde\mu(\tau)]\\\
&=\mathbb E_{\tau\sim p}[-\partial_\psi r_\psi(\tau)]+\mathbb E_{\tau\sim\mu}\left[{ {1\over Z}\exp(r_\psi(\tau))\partial_\psi r_\psi(\tau)\over\tilde\mu(\tau)}\right]\\\
&=-\mathbb E_{\tau\sim p}[\partial_\psi r_\psi(\tau)]+{1\over Z}\mathbb E_{\tau\sim\mu}\left[{\exp(r_\psi(\tau))\partial_\psi r_\psi(\tau)\over\tilde\mu(\tau)}\right]\\\
&=\partial_\psi\mathcal L_r(\psi)
\end{align}
$$

where in the first step we omit terms irrelevant to $$\psi$$. The last equality holds because $$Z$$ in the discriminator estimates the partition function in the MaxEnt IRL objective and we do not differential through the importance weights when we compute the derivative of the MaxEnt IRL objective. This result suggests that applying gradient descent to the discriminator in GAN is equal to doing the same to the MaxEnt IRL objective.

### The Generator Optimizes The Soft Optimal Policy

Remember that we assume the generator density to be $$q_\theta(\tau)$$. Now we compute the generator's loss defined in Eq.$$(2)$$:

$$
\begin{align}
\mathcal L_{G}(\theta)&=\mathbb E_{\tau\sim q}[-\log D(\tau)+\log(1-D(\tau))]\\\
&=\mathbb E_{\tau\sim q}\left[-\log{ {1\over Z}\exp(r_\psi(\tau))\over 2\tilde \mu}+\log{q_\theta(\tau)\over 2\tilde\mu}\right]\\\
&=\log Z+\mathbb E_{\tau\sim q}\left[\log q_\theta(\tau)-r_\psi(\tau)\right]\\\
&=\log Z+\mathcal L_q(\theta)
\end{align}
$$

Since $$Z$$ is trained with the discriminator that is fixed while optimizing the generator, this loss is exactly equivalent to the policy loss Eq.$$(4)$$ from MaxEnt IRL.

### Summary

With all the equipment at our disposal, now let us fomulate the algorithm

$$
\begin{align}
&\mathbf{GAN\_GCL:}\\\
&\quad \mathrm{Initialize\ policy\ }\pi_\theta\ \mathrm{and\ discriminator\ }\mathcal D_{\psi}\\\
&\quad \mathbf{For}\ i=1\mathrm{\ to\ }N:\\\
&\quad\quad \mathrm{Generate\ samples\ }\mathcal D_{samp}\ \mathrm{from\ }\pi_\theta\\\
&\quad\quad \mathrm{Sample\ expert\ demonstration}\ \mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ }\psi\ \mathrm{in\ discriminator}\ D_\psi\mathrm{\ using\ }\mathcal D_{samp}\mathrm{\ and\ }\mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ \theta\ in\ policy\ }\pi_\theta\mathrm{\ using\ }\mathcal D_{samp}
\end{align}
$$


As the authors of AIRL answered [here](https://openreview.net/forum?id=rkHywl-A-&noteId=S1Nj--xSG): in practice, $$Z$$ cannot be learned uniquely, and should be considered as a part of the discriminator.

## Adversarial Inverse Reinforcement Learning

In GAN-GCL, we use full trajectories as GCL does, which would result in high variance and very poor learning. Moreover, the learned reward may be a shaped reward, which may not be robust to changes in dynamics. To see this, consider deterministic dynamics $$\mathcal T(s,a)\rightarrow s'$$ and state-action rewards $$\hat r(s,a)=r(s,a)+\gamma\Phi(\mathcal T(s,a))-\Phi(s)$$. It is easy to see that changing the dynamics $$\mathcal T$$ to $$\mathcal T'$$ such that $$\mathcal T'(s,a)\ne\mathcal T(s,a)$$ means that $$\hat r(s,a)$$ is no longer shaped in the same way as before. 

Justin Fu et al. in 2018 proposed an algorithm named **A**dversarial **I**nverse **R**einforcement **L**earning (AIRL), which improves GAN-GCL by using single-step transitions instead to reduce variance and employing an additional shaping term to mitigate the effects of unwanted shaping.

Formally, they propose the discriminator as

$$
D_{\psi, \phi}(s,a,s')={\exp(f_{\psi,\phi}(s,a,s'))\over \exp(f_{\psi,\phi}(s,a,s'))+\pi(a|s)}\tag {9}
$$

where the normalization constant $$Z$$ has been folded into $$f_{\psi,\phi}$$ as [this answer](https://openreview.net/forum?id=rkHywl-A-&noteId=S1Nj--xSG) suggests, and $$f_{\psi, \phi}$$, serving as both the shaped reward function and the advantage function, is defined as

$$
f_{\psi,\phi}(s,a,s')=g_\psi(s,a)+\gamma h_\phi(s')-h_\phi(s)
$$

where, ideally, $$g_\psi$$ is optimized to be the ground truth reward function of the state plus some constant, $$h_\phi$$ is optimized to be the optimal state value function plus some constant. As a shaping function, $$h_\phi$$ helps mitigate the effects of unwanted shaping on the reward approximator $$g_\psi$$.

When $$f_{\psi,\phi}$$ is regarded as the advantage function, $$\exp(f_{\psi,\phi}(s,a,s'))$$ serves as an estimate of the data policy. Therefore, Eq.$$(9)$$ could be intuitively thought of as a simplification of Eq.$$(5)$$ in single time step.

### Algorithm


$$
\begin{align}
&\mathbf{AIRL:}\\\
&\quad \mathrm{Initialize\ policy\ }\pi_\theta\ \mathrm{and\ discriminator\ } D_{\psi,\phi}\\\
&\quad \mathbf{For}\ i=1\mathrm{\ to\ }N:\\\
&\quad\quad \mathrm{Generate\ samples\ }\mathcal D_{samp}\ \mathrm{from\ }\pi_\theta\\\
&\quad\quad \mathrm{Sample\ expert\ demonstration}\ \mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ }\psi\ \mathrm{and}\ \phi\ \mathrm{in\ discriminator}\ D_{\psi,\phi}\mathrm{\ using\ }\mathcal D_{samp}\mathrm{\ and\ }\mathcal D_{demo}\\\
&\quad\quad \mathrm{Update\ \theta\ in\ policy\ }\pi_\theta\mathrm{\ using\ }\mathcal D_{samp}
\end{align}
$$


This algorithm bears much resemblance to GAN-GCL, with two difference: 1. we define the discriminator as $$D_{\psi,\phi}$$ in Eq.$$(8)$$, and 2. we update the discriminator using transitions instead of trajectories.

## References

1. CS 294-112 at UC Berkeley. Deep Reinforcement Learning Lecture 16
2. Chelsea Finn et al. A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models
3. Ian J. Goodfellow et al. Generative Adversarial Nets.
4. Justin Fu et al. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning
5. Andrew Ng et al. Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping