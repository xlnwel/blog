---
title: "CQL — Conservative Q-Learning"
excerpt: "In which we discuss an offline RL method that learns a conservative Q function to avoid overestimation on out-of-distribution actions."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Offline Reinforcement Learning
---

## Introduction

In offline RL, a policy learned by off-policy RL such as Q-learning may suffer from distribution shift at test time. This happens because a policy may be bias towards out-of-distribution actions with erroneously high Q-values. In online RL, such errors can be corrected by attempting the action it believes to be good and observing it's not. However, this is not feasible in offline setting where interaction with the environment is not allowed. Previous methods such as [BCQ]({{ site.baseurl }}{% post_url 2019-12-07-BCQ %}) mitigates this problem by constraining the learned policy away from out-of-distribution actions. In this post, we discuss [Conservative Q-Learning](CQL), such that the expected value of a policy under the learned Q-function lower bounds its true value.

## CQL

CQL optimizes the following objective

$$
\begin{align}
\min_\theta\max_\phi\ &\alpha\left(\mathbb E_{s\sim\mathcal D,a\sim\pi_\phi}[Q_\theta(s,a)]
-\mathbb E_{s,a\sim\mathcal D}[Q_\theta(s,a)]\right)\\\
&+{1\over2}\mathbb E_{s,a,s'\sim\mathcal D}\left[\left(Q_\theta(s,a)-(r(s,a)+\gamma\mathbb E_{a'\sim\pi_{\phi'}}[Q_{\theta'}(s',a')])\right)^2\right]\\\
&+\mathcal R(\pi_\phi)\tag 1
\end{align}
$$

where $$\pi_\phi$$ and $$Q_\theta$$ are the policy and $$Q$$-function to learn, $$\pi_{\phi'}$$ and $$Q_{\theta'}$$ are the target policy and $$Q$$-function that periodically updated from $$\pi_\phi$$ and $$\Q_\theta$$, respectively. Kumar et al. theoretically shows that the expected value learned from Equation $$(1)$$ lower-bounds its true value, which we do not prove here, Instead, we intuitively analyze each term in Equation $$(1)$$ below. 

Equation $$(1)$$ contains four terms: 

- $$\mathbb E_{s\sim\mathcal D,a\sim\pi_\phi}[Q_\theta(s,a)]$$: When we optimize this term w.r.t. $$\theta$$, we penalize the $$Q$$ function under the current policy $$\pi_\phi$$. This ensures the learned policy is conservative under policy $$\pi_\phi$$, preventing erroneously large $$Q$$ values on out-of-distribution actions. On the other hand, optimizing this term w.r.t. $$\phi$$ improves the performance of $$\pi_\phi$$ as done in standard actor-critic architecture.
- $$\mathbb E_{s,a\sim\mathcal D}[Q_\theta(s,a)]$$: Simply optimizing the above term w.r.t $$\theta$$ will cause an excessive underestimation, especially as $$\pi_\phi$$ becomes (near)-optimal. Therefore, we additionally maximize the $$Q$$ function under the behavior policy w.r.t. $$\theta$$ to mitigate the potential risk of underestimation.
- $${1\over2}\mathbb E_{s,a,s'\sim\mathcal D}\left[\left(Q_\theta(s,a)-(r(s,a)+\gamma\mathbb E_{a'\sim\pi_{\phi'}}[Q_{\theta'}(s',a')])\right)^2\right]$$: It's just the standard mean squared error for $$Q$$-learning. Notice that if we don't learn a policy, $$\pi_{\phi'}$$ is the $$argmax$$ operator.
- $$\mathcal R(\pi_\phi)$$: This term aims to regularize the learned policy. It could be KL-divergence against a prior distribution, which gives us the exact same policy objective in maximum entropy reinforcement learning such as SAC. 

## Experimental Results

<figure>
  <img src="{{ '/images/brl/CQL-Table1.png' | absolute_url }}" alt="" width="1000">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

Table 1 shows that CQL roughly matches the previous methods when the dataset generated from a single policy. However, on dataset generated by mixed policies, CQL outperforms prior methods by a large margins.

## Thoughts

When dataset are generated by (near-)optimal policy, we could replace $$\mathcal R(\pi_\phi)$$ with the behavior cloning loss.

## References

Kumar, Aviral, Aurick Zhou, George Tucker, and Sergey Levine. 2020. “Conservative Q-Learning for Offline Reinforcement Learning.” *ArXiv*.