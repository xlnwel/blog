---
title: "MPO — Maximum a Posteriori Policy Optimization"
excerpt: "In which we discuss maximum a posteriori policy optimization, a KL-regularized reinforcement learning method."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Model-Free Reinforcement Learning
  - Regularization in Reinforcement Learning
---

## Introduction

We discuss Maximum a posteriori Policy Optimization, a KL-regularized reinforcement learning algorithm for continuous control problems. Despite its appealing performance on difficult control task, MPO is a bit involved compared to contemporary SOTA methods such as SAC and TD3. Nevertheless, it introduces several interesting techniques that may benefit other algorithms.

## Background

In our previous posts[[1]({{ site.baseurl }}{% post_url 2019-01-14-PGM %}), [2]({{ site.baseurl }}{% post_url 2019-01-21-SVI %})], we considered control problems as a temporal probabilistic graphical model, in which we introduced optimality variable $$O_t$$ that indicated whether the corresponding state and action were optimal. We then derived an evidence lower bound on the likelihood of optimality $$\log(p(O_{1:T}))$$ via variational inference. We briefly repeat the process as follows

$$
\begin{align}
\log p(O_{1:T})&=\log{p(O_{1:T}|\tau)p(\tau)q(\tau)\over p(\tau|O_{1:T}) q(\tau)}\\\
&\qquad\color{red}{\text{take expectation over }q(\tau) \text{ and rearange}}\\\
&=\mathbb E_{q(\tau)}[\log p(O_{1:T}|\tau)]-D_{KL}(q(\tau)\Vert p(\tau))+D_{KL}(q(\tau)\Vert p(\tau|O_{1:T}))\\\
&\ge\mathbb E_{q(\tau)}[\log p(O_{1:T}|\tau)]-D_{KL}(q(\tau)\Vert p(\tau))\\\
&\qquad\color{red}{\text{expand trajectory, omit transition probabilities, and rearange}}\\\
&=\sum_t\mathbb E_{s_t,a_t\sim q}\left[\log p(O_t|s_t,a_t)\right]-D_{KL}(q(a_t| s_t)\Vert p(a_t|s_t))\\\
&\qquad\color{red}{p(O_t|s_t,a_t)\propto\exp(r(s_t,a_t))}\\\
&=\sum_t\mathbb E_{s_t,a_t\sim q}\left[r(s_t,a_t)\right]-D_{KL}(q(a_t| s_t)\Vert p(a_t|s_t))
\end{align}
$$


Adding discount factor $$\gamma$$ and temperature $$\alpha$$, we obtain the KL regularized RL objective as follows:


$$
\begin{align}
\max_q\mathcal J(q,p)=\max_q \mathbb E_{q}[\sum_{t=0}^\infty\gamma^t\big(r(s_t,a_t)-\alpha D_{KL}(q(a_t|s_t)\Vert p(a_t|s_t))\big)]\tag{1}\label{eq:1}
\end{align}
$$


Define $$V(s_t)=\mathbb E_{q}[\sum_{t'\ge t}\gamma^{t'-t}\big(r(s_{t'},a_{t'})-\alpha D_{KL}(q(a_{t'}\vert s_{t'})\Vert p(a_{t'}\vert s_{t'}))\big)]$$ and $$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb E_{s_{t+1}\sim p(s_{t+1}\vert s_t,a_t)}[V(s_{t+1})]$$. We can rewrite Equation $$\eqref{eq:1}$$ as


$$
\begin{align}
\max_q\mathcal J(q,p)=\max_q \mathbb E_q[Q(s_t,a_t)-\alpha D_{KL}(q(a_t|s_t)\Vert p(a_t|s_t))]\tag{2}\label{eq:2}
\end{align}
$$

Note that given the prior policy $$p$$ and action value function $$Q$$, we can derive the optimal policy $$q$$ in a close form as $$q(a\vert s)\sim p(a\vert s)\exp(Q(s,a)/\alpha)$$. Therefore, MPO alternates between evaluating $$Q$$ and optimizing $$p$$, where $$p$$ can be regard as $$q$$ in the previous step. This process can be regarded as an instance of the family of [Expectation Maximization]({{ site.baseurl }}{% post_url 2018-12-28-EM %})(EM) algorithms. We elaborate MPO in the following sections following the EM framework.

## E-Step

In the E-step of iteration $$i$$, we first evaluate $$Q$$ with $$p=q=\pi_i$$, where $$\pi_i$$ is the policy learned in the previous iteration. This reduces the regularized $$Q$$ function to the standard $$Q$$ function: $$Q(s_t,a_t)=\sum_{t\ge t}\gamma^{t'-t}r(s_t,a_t)$$ and enables us to optimize $$Q$$ by minimizing the Bellman residual -- in practice, the target $$Q$$ is estimated from the Retrace algorithm. 

## M-Step

In the M-step of iteration $$i$$, we update $$\pi_i$$ to $$\pi_{i+1}$$. This can be done in two ways depending on which one of $$p$$ and $$q$$ we want to optimize.

### Optimizing $$q=\pi$$ 

In this way, we optimize Equation $$\eqref{eq:2}$$ with $$p=\pi_i$$, i.e.,

$$
\begin{align}
q=\arg\max_q{1\over N}\sum_{s\sim\mathcal D}\sum_{a\sim q(a|s)} q(a|s)[Q(s,a)-\alpha D_{KL}(q(a|s)\Vert p(a|s))]\tag{3}\label{eq:3}
\end{align}
$$

where $$N$$ is the total number of $$(s,a)$$ pairs used in a minibatch. 

### Optimizing $$p=\pi$$ 

Notice that the optimal solution of $$q$$ for Equation $$\eqref{eq:2}$$ is $$q(a\vert s)\sim p(a\vert s)\exp(Q(s,a)/\alpha)$$. Therefore, we compute a sample-based policy $$q(a_i\vert s_i)={p(a_i\vert s_i)\exp(Q(s_i,a_i))\over\sum_jp(a_j\vert s_j)\exp(Q(s_j,a_j))}$$. This enables us to optimize $$p$$ by minimizing the KL divergence between $$q$$ and $$p$$

$$
\begin{align}
p=\arg\min_pD_{KL}(q\Vert p)=\arg\max_p{1\over N}\sum_{s\sim\mathcal D}\sum_{a\sim q(a|s)}q(a|s)\log  p(a|s)\tag{4}\label{eq:4}
\end{align}
$$

Unfortunately, sample based maximum likelihood can suffer from overfitting to samples. Additionally, $$q(a\vert s)$$ is unreliable due to a poor approximation of $$Q$$ -- potentially resulting in a large change of the action distribution in the wrong direction when optimizing Equation $$\eqref{eq:4}$$. One effective regularization that address both concerns is to limit the overall change in $$q$$. This is done by adding an additional KL constraint and changing the objective from Equation $$\eqref{eq:4}$$ to

$$
\begin{align}
p=&\arg\max_p{1\over N}\sum_{s\sim\mathcal D}\sum_{a\sim q(a|s)}q(a|s)\log  p(a|s)\tag{5}\label{eq:5}\\\
s.t.&\quad D_{KL}(\pi_i\Vert p)\le \epsilon_\pi
\end{align}
$$

To make objective amenable to gradient based optimization we employ Lagrangian Relaxation, yielding the following primal optimization problem

$$
\begin{align}
\max_p\min_\lambda{1\over N}\sum_{s\sim\mathcal D}\sum_{a\sim q(a|s)}q(a|s)\log  p(a|s)+\lambda (\epsilon_\pi-{1\over N}\sum_{i}^KD_{KL}(\pi_i(\cdot|s_i)\Vert p(\cdot|s_i))\tag{6}\label{eq:6}
\end{align}
$$

We solve the objective from Equation $$\eqref{eq:6}$$ by iteratively optimizing $$p$$ and $$\lambda$$ independently.

Moreover, [Abdolmaleki et al. 2018ab](#ref2) find it empirically better to replace the soft KL regularization in Equation $$\eqref{eq:2}$$ with a hard constraint

$$
\begin{align}\max_q\mathcal J(q,p)=\max_q \mathbb E_q[Q(s_t,a_t)]\tag{7}\label{eq:7}\\\
s.t.\quad D_{KL}(q(a_t|s_t)\Vert p(a_t|s_t))<\epsilon
\end{align}
$$

This changes the optimal $$q$$ to $$q(a\vert s)\sim \exp(Q(s,a)/\eta)$$, where $$\eta$$ can be found by solving the following convex dual function(proof in Supplementary Materials)

$$
\begin{align}
\eta=\arg\min_\eta\eta\epsilon+\eta\sum_j^k{1\over K}\log\left(\sum_i^N{1\over N}\exp(Q(s_j,a_i)/\eta)\right)\tag{8}\label{eq:8}
\end{align}
$$

In Section 4.2.1 of [Abdolmaleki et al. 2018b](#ref2), an additional improvement for Gaussian policy is introduced to further prevent from premature convergence. The solution is simple but I cannot make sense of the motivation. I refer interested readers to the paper for more details.

[Song et al. 2019](#ref3) also find it is better to optimize policy $$q$$ and $$\eta$$ with only top $$50\%$$ advantages data.

## References

<a name='ref1'></a>Abdolmaleki, Abbas, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. 2018. “Maximum a Posteriori Optimization.”

<a name='ref2'></a>Abdolmaleki, Abbas, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan Belov, Nicolas Heess, and Martin Riedmiller. 2018. “Relative Entropy Regularized Policy Iteration.” *ArXiv*.

<a name='ref3'></a>Song, H. Francis, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, et al. 2019. “V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control,” 1–19. http://arxiv.org/abs/1909.12238.

## Supplementary Materials

### Dual Function Derivation

When we optimizing $$p=\pi$$, we solve

$$
\begin{align}
\max_q\mathcal J(q,p)=&\max_q \int_s\mu(s)\int_a q(a|s)Q(s_t,a_t)\\\
s.t.\quad\int_s \mu(s)D_{KL}(q(a|s)\Vert p(a|s))<&\epsilon\\\
\int_s\int_a\mu(s)q(a|s)=&1
\end{align}
$$

where $$\mu(s)$$ is the state distribution.

First we write the Lagrangian

$$
\begin{align}
\mathcal L(q,\eta,\nu)=&\int_s\mu(s)\int_a q(a|s)Q(s,a)\\\
&+\eta\left(\epsilon-\int_s \mu(s)\int_aq(a|s)\log {q(a|s)\over p(a|s)}\right)\\\
&+\nu\left(1-\int_s\int_a\mu(s)q(a|s)\right)
\end{align}
$$

Taking the derivative w.r.t. $$q$$ yields

$$
\begin{align}
{\partial\over\partial q}\mathcal L(q,\eta,\nu)=Q(s,a)-\eta\log q(a|s)+\eta\log p(a|s)-(\eta+\nu)
\end{align}
$$

Setting it to zero and rearranging terms, we get

$$
\begin{align}
q(a|s)=p(a|s)\exp(Q(s,a)/\eta)\exp(-(\eta+\nu)/\eta)\tag{9}\label{eq:9}
\end{align}
$$

The last term is independent of $$a$$ and is a normalization constant for $$q$$. Therefore, we have

$$
\begin{align}
\exp\left({\eta+\nu\over\eta}\right)=\int_a p(a|s)\exp(Q(s,a)/\eta)\\\
{\eta+\nu\over\eta}=\log\int_a p(a|s)\exp(Q(s,a)/\eta)
\end{align}
$$

Now we plug in Equation $$\eqref{eq:9}$$ to the log likelihood ratio of the Lagrangian and it results in 

$$
\begin{align}
\mathcal L(q,\eta,\nu)=&\int_s\mu(s)\int_a q(a|s)Q(s,a)\\\
&+\eta\left(\epsilon-\int_s \mu(s)\int_aq(a|s)\left({Q(s,a)\over\eta}-{(\eta-\nu)\over\eta}\right)\right)\\\
&+\nu\left(1-\int_s\int_a\mu(s)q(a|s)\right)\\\
=&\eta\epsilon+\eta{\eta-\nu\over\eta}\\\
=&\eta\epsilon+\eta\log\int_a p(a|s)\exp(Q(s,a)/\eta)
\end{align}
$$
