---
title: "QMIX, WQMIX and More"
excerpt: "Discussion on QMIX, WQMIX and some tricks on QMIX."
categories:
  - Reinforcement Learning
tags:
  - Multi-Agent RL
---

## Introduction

QMIX is a value based algorithm for multi-agent setting. In a nutshell, QMIX learns an agent-specific $$Q$$ network from the agent’s local observation and combines them via a monotonic mixing network whose weights are generated by small networks with an absolute activation in the end.

## Assumption

QMIX is built upon the assumption that a global $$\arg\max$$ performed on $$Q_{tot}$$ yields the same result as a set of individual $$\arg\max$$ operations performed on each $$Q_a$$. To this end, it suffices to enforce a monotonicity constraint on the relationship between $$Q_{tot}$$ and each $$Q_a$$

$$
\begin{align}
{\partial Q_{tot}\over\partial Q_a}\ge 0\quad \forall a
\end{align}
$$

Note that this assumption is not always hold. In cases where an agent's decision should be made according to other agents' action at the same time step, this assumption breaks.

## Method

<figure>
  <img src="{{ '/images/marl/qmix-Figure2.png' | absolute_url }}" alt="" width="1000">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

QMIX follows the *centralized training and decentralized execution*(CTDE) paradigm to allow agents to run separately while training jointly. As a result, QMIX consists of agent networks that represents each $$Q_a$$ and a mixing network that combines $$Q_a$$s into $$Q_{tot}$$. 

As illustrated in Figure2, agent network is simply a feed-forward network, optionally including a recurrent unit, that maps the observation and previous action of agent $$a$$ to a utility function $$Q_a(\tau^a,\cdot)$$. After applying softmax to produce the maximum utility $$Q_a(\tau^a, u_t^a)$$, we feed all utilities to a mixing network and compute $$Q_{tot}(\pmb \tau,\pmb u)$$. In order to enforce the monotonicity constraint, all weights in the mixing network should be non-negative. This is achieved by separate hypernetworks, which takes the global state $$s$$ as input and generate the weights of each layer of the mixing network. The output of the hypernetwork is a vector, which is shaped into a matrix of appropriate size. Finally, we apply an absolute activation to ensure non-negativity for weights (but not biases).

## Tricks

Several algorithms have been trying to relax the strong assumption made by QMIX. However, [Hu et al. 2021](#ref2) find these methods may not always yield a better performance. Instead, they discuss several tricks on QMIX that significantly improve the performance. We distill three of them bellow

1. Use Adam instead RMSProp. Adam boosts the network convergence and is more likely to benefit from distributed training.
2. Use $$Q(\lambda)$$ with a small value of $$\lambda$$, where $$Q(\lambda)=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_n$$, in which $$G_n$$ is the discounted n-step return. Interestingly, they don't use Retrace here.
3. Small replay buffer. They provides three reasons for this phenomenon: 1) In multi-agent tasks, samples become obsolete faster than in single-agent task. 2) Adam performs better with sample with fast updates. 3) $$Q(\lambda)$$ converges more effectively with new samples. I suspect the main reason is that $$Q(\lambda)$$ is strongly biased for off-policy data. 

## References

<a name="ref1"></a>Rashid, Tabish, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2018. “QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” *35th International Conference on Machine Learning, ICML 2018* 10: 6846–59.

<a name="ref2"></a>Hu, Jian, Haibin Wu, Seth Austin Harding, and Shih-wei Liao. 2021. “Hyperparameter Tricks in Multi-Agent Reinforcement Learning: An Empirical Study.” http://arxiv.org/abs/2102.03479.