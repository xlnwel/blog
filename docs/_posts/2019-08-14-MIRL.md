---
title: "MIRL — Mutual Information Reinforcement Learning"
excerpt: "In which we discuss a new regularization mechanism that leverage an optimal prior to explicitly penalize the mutual information between states and actions."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Model-Free Reinforcement Learning
---

## Introduction

Energy-based regularization has previously shown to improve both exploration and robustness in changing sequential decision-making tasks. It does so by encouraging policies to spread probability mass on all actions equally. However, entropy regularization might be undesirable when actions have significantly different importance. For example, some actions may be useless in certain tasks and uniform actions in this case would introduce fruitless exploration. Jordi Grau-Moya et al. in ICLR 2019 propose a novel regularization that dynamically weights the importance of actions(i.e. adjusts the action prior) using mutual information. 

## Derivation of Mutual-Information Regularization

In the [previous post]({{ site.baseurl }}{% post_url 2019-01-21-SVI %}), we framed the reinforcement learning problem as an inference problem and obtain the soft optimal objective as follows

$$
\begin{align}
\max_{p,q}\mathcal J(p, q)=\sum_{t=1}^T\gamma^{t-1}\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)-\tau\log{q(a_t|s_t)\over p(a_t)}\right]\tag{1}\label{eq:1}
\end{align}
$$

where $$\tau$$ is the temperature, $$q(a_t\vert s_t)$$ is the action distribution of the policy to optimize, and the action prior $$p(a_t)$$ is put back since we no longer assume it's a uniform distribution here. Moving the expectation inward, we obtain

$$
\begin{align}
\max_{p,q}\mathcal J(p,q)=&\sum_{t=1}^T\gamma^{t-1}\left\{\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)\right]-\tau\mathbb E_{s_{t}, a_{t}\sim q}\left[\log{q(a_t|s_t)\over p(a_t)}\right]\right\}\\\
=&\sum_{t=1}^T\gamma^{t-1}\left\{\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)\right]-\tau\sum_{s_t} q(s_t)D_{KL}(q(a|s_t)\Vert p(a))\right\}\\\
=&\sum_{t=1}^T\gamma^{t-1}\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)\right]-\tau\sum_sq(s)D_{KL}(q(a|s)\Vert p(a))\tag{2}\label{eq:2}\\\
\overset{\mathrm{if}\ p(a)=q(a)}{=}&\sum_{t=1}^T\gamma^{t-1}\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)\right]-\tau I_{q}(S;A)\tag{3}\label{eq:3}\\\
where\quad q(s)=&{1\over Z}\sum_{t=1}^T\gamma^{t-1}\sum_{s_t}q(s_t)\tag{4}\label{eq:4}\\\
=&{1\over Z}\sum_{t=1}^T\gamma^{t-1}\sum_{s_1,a_1,\dots,s_{t-1},a_{t-1}}q(s_1)\left(\prod_{t'=0}^{t-2}q(a_t'|s_t')P(s_{t'+1}|s_{t'},a_{t'})\right)q(a_{t-1}|s_{t-1})P(s|s_{t-1},a_{t-1})
\end{align}
$$

Here, $$q(s)$$ is the discounted marginal distribution over states. We kind of abuse the equality sign in Equation $$\eqref{eq:2}$$ as we implicitly add the partition function $$1\over Z$$ to $$q(s)$$. This, however, does not change our optimization objective since it's a constant and can easily be corrected through $$\tau$$. 

From Equation $$\eqref{eq:2}$$,  we can see that maximizing $$\mathcal J(p,q)$$ w.r.t. $$p$$ equally minimizes $$\sum_sq(s)D_{KL}(q(a\vert s)\Vert p(a))$$, which is minimized when $$p(a)=q(a)$$:

$$
\begin{align}
&q(s)D_{KL}(q(a|s)\Vert p(a)) - q(s)D(q(a|s)\Vert q(a))\\\
=&\sum_{s,a}q(s,a)\left(\log{q(a|s)\over p(a)}-\log {q(a|s)\over q(a)}\right)\\\
=&\sum_{s,a}q(s,a)\log{q(a)\over p(a)}\\\
=&\sum_{a}q(a)\log{q(a)\over p(a)}\\\
=&D_{KL}(q\Vert p)\\\
\ge&0
\end{align}
$$

where the last inequality holds because of the non-negativity of the KL divergence.

Equation.$$\eqref{eq:3}$$ suggests that when the action distribution of the current policy is taken as the action prior, our soft optimal objective now penalizes the mutual information between state and action instead of maximizing the policy entropy! Intuitively, this means that we want to discard information in $$s$$ irrelevant to the agent's performance.

With the form of the optimal prior for a ﬁxed policy at hand, one can easily devise a stochastic approximation method (e.g. $$p(a)=(1-\alpha)p(a)+\alpha q(a\vert s)$$ with $$\alpha\in[0,1]$$ and $$s\sim q(s)$$) to estimate the optimal prior $$p(a)$$ from the current estimate of the optimal policy $$q(a\vert s)$$. 

## Mutual Information Reinforcement Learning Algorithm

Now we apply mutual information regularization to deep Q-networks(DQN). The algorithm,Mutual Information Reinforcement Learning(MIRL), makes five updates to traditional DQN:

**Initial Prior Policy**: For a initial fixed policy $$q(a)$$, we compute $$p(a)$$ by minimizing $$\sum_{s}q(s)D_{KL}(q(a\vert s)\Vert p(a))$$. 

**Prior Update**: We approximate the optimal prior by employing the following update equation

$$
\begin{align}
p(a)=(1-\alpha_p)p(a)+\alpha_p q(a|s)
\end{align}
$$

where $$s\sim q(s)$$ and $$\alpha_p$$ is the step size.

**Q-function Updates**: Concurrently to learning the prior, MIRL optimizes $$Q$$-function by minimizing the following loss

$$
\begin{align}
L(\theta)&:=E_{s,a,r,s'\sim replay}\left[\left((\mathcal T_{soft}^pQ)(s,a,s';\theta^-)-Q(s,a;\theta)\right)^2\right]\tag{13}\label{eq:13}\\\
where\quad (\mathcal T_{soft}^pQ)(s,a,s';\theta^-)&:=r(s,a)+\gamma{\tau}\log\sum_{a'}p(a')\exp( Q(s',a';\theta^-)/\tau)
\end{align}
$$

where the second term of the soft $$Q$$ target computes soft value function at the next state $$V_{soft}^p(s')$$. Although this target $$Q$$ meets the definition of the soft Bellman equation, it is somewhat comfusing if we follow the derivation of soft value iteration. 

**Behavioral Policy**: MIRL's behavioral policy consists of two parts: when exploiting, it takes greedy action based on the soft optimal policy; when exploring, it follows the optimal prior distribution. Mathematically, given a random sample $$u\sim \mathrm{Uniform}[0,1]$$ and epsilon $$\epsilon$$, the action is obtained by

$$
\begin{align}
a=
\begin{cases}
\arg\max_a\pi(a|s)&\mathrm{if}\ u>\epsilon\\\
a\sim p(\cdot)&\mathrm{if}\ u\le\epsilon
\end{cases}\\\
where\quad \pi(a|s)={1\over Z}p(a)\exp(\beta Q(s,a))
\end{align}
$$

**$$\tau$$ Update**: the temperature $$\tau$$ controls penalty for the mutual information between state and action. As one might expect, it should be large at first and gradually anneals down during training process to ensure initial exploration. The authors define an update rule for the inverse of $$\tau$$, $$\beta={1\over\tau}$$, as follows

$$
\begin{align}
\beta=(1-\alpha_\beta)\beta+\alpha_{\beta}\left({1\over L(\theta)}\right)
\end{align}
$$


Where $$\alpha_\beta$$ is the step size

### Algorithm

Now it is straightforward to see the whole algorithm

<figure>
  <img src="{{ '/images/soft optimality/MIRL.png' | absolute_url }}" alt="" width="1000">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

## Discussion

Back when we discussed Eq.$$\eqref{eq:3}$$, we said that 

> when the action distribution of the current policy is taken as the action prior, our soft optimal objective now penalizes the mutual information between state and action instead of maximizing the policy entropy. 

In fact, this objective can be easily optimized through a regular RL algorithm plus a mutual information network, such as MINE, DIM.

### References

Grau-Moya, Jordi, Felix Leibfried, and Vrancx Peter. 2019. “Soft Q-Learning with Mutual-Information Regularization.” *ICLR 2019*, 1–9.
