---
title: "GAIL — Generative Adversarial Imitation Learning"
excerpt: "A GAN-style imitation learning method"
categories:
  - Reinforcement Learning
tags:
  - Inverse RL
---

##  Introduction

Different from the paper, we in this post adopt the reward maximization paradigm rather than the cost minimization framework.

## Preliminaries

### MaxEnt IRL

In MaxEnt IRL, we define the following optimization problem

$$
\begin{align}
\min_r\max_\pi\mathbb E_\pi[r(s,a)]+\mathcal H(\pi)-\mathbb E_{\pi_E}[r(s,a)]\tag{1}\label{eq:1}
\end{align}
$$

where we omit domains whenever it does not cause any ambiguity. Because IRL learned from a finite expert dataset can easily overfit, we introduce a convex regularizer: \\(\psi:\mathbb R^{\mathcal S\times\mathcal A}\rightarrow \mathbb R\\). Now we define the regularized IRL objective

$$
\begin{align}
\text{IRL}_\psi(\pi_E)=\arg\min_r\psi(r)+(\max_\pi\mathbb E_\pi[r(s,a)]+\mathcal H(\pi))-\mathbb E_\pi[r(s,a)]\tag{2}\label{eq:2}
\end{align}
$$


### Occupancy measure

For a policy \\(\pi\in \Pi\\), define its occupancy measure \\(\rho_\pi:\mathcal S\times\mathcal A\rightarrow \mathbb R\\) as \\(\rho_\pi(s,a)=\pi(a\vert s)\sum_{t=0}^\infty\gamma^t P(s_t=s\vert \pi)\\), which is the discounted state-action visitation distribution of policy \\(\pi\\). It allows us to write \\(\mathbb E_\pi[r(s,a)]=\sum_{s,a}\rho_\pi(s,a)r(s,a)\\) for any reward function \\(r\\). Define the set of valid occupancy measure \\(\mathcal D=\{\rho_\pi:\pi\in\Pi\}\\). It's easy to see that there is a one-to-one correspondence between \\(\Pi\\) and \\(\mathcal D\\). Furthermore, we have

1. \\(\pi_\rho=\rho(s,a)/\sum_{a'}\rho(s,a')\\).
2. \\(\rho(s,a)=\pi_\rho(a\vert s)\rho(s)\\), where \\(\rho(s)\\) is the state visitation distribution.
3. \\(\mathcal H(\pi)=\bar{\mathcal H}(\rho_\pi)\\) and \\(\bar{\mathcal H}(\rho)=\mathcal H(\pi_\rho)\\), where \\(\bar{\mathcal H}(\rho)=-\sum_{s,a}\rho(s,a)\log\pi_\rho(s,a)\\) and \\(\mathcal H(\pi)=-\sum_s\rho_\pi(s)\sum_a\pi(a\vert s)\log\pi(s,a)\\)

## Characterizing the induced optimal policy

Let \\(\text{RL}(r)\\) be the policy learned by RL with the reward function \\(r\\) and \\(\text{IRL}(\pi_E)\\) be the reward function recovered by IRL from expert policy \\(\pi_E\\).

**Proposition 3.2** \\(\text{RL}\circ\text{IRL}(\pi_E)=\arg\max_{\pi}\mathcal H(\pi)-\psi^*(\rho_\pi-\rho_E)\\).

The proof is given [here](#proposition3.2). Proposition 3.2 tells us that \\(\psi\\)-regularized IRL implicitly seeks a policy whose occupancy measure is close to the expert's. Enticingly, this suggests that various settings of \\(\psi\\) lead to various imitation learning algorithms that directly solve the optimization problem given by Proposition 3.2.

**Corollary 3.2.1** If \\(\psi\\) is a constant function, \\(\tilde c\in\text{IRL}(\pi_E)\\), and \\(\tilde \pi\in\text{RL}(\tilde c)\\), then \\(\rho_{\tilde \pi}=\rho_{E}\\).

**Proof.** We use \\(\bar{\mathcal L}(\rho,r)=\bar {\mathcal H}(\rho)-\sum_{s,a}(\rho(s,a)-\rho_{\pi_E}(s,a))r(s,a)\\) from the proof of Proposition 3.2 in the following proof.

We can see that  \\(\tilde r=\arg\min_r\max_\rho\bar{\mathcal L}(\rho,r)\\) is the dual of the following optimization problem

$$
\begin{align}
\max_\rho\bar{\mathcal H}&(\rho)\tag{3}\label{eq:3}\\\
s.t.\quad \rho(s,a)=&\rho_E(s,a)\quad\forall s\in\mathcal S,a\in\mathcal A
\end{align}
$$

with \\(\tilde r\\) being the dual optimum. Because of the strictly concavity of \\(\bar{\mathcal H}\\), the primal optimum is obtained when the constraint is satisfied.

From corollary 3.2.1, we deduce that

1. IRL is a dual of an occupancy measure matching problem
2. The induced optimal policy is the primal optimum.

## Method

Corollary 3.2.1 is not useful in practice as, in reality, the expert trajectory distribution is unknown and instead is often provided as a finite set of samples. Therefore, we relax Equation \\(\eqref{eq:3}\\) into the following form, motivated by Proposition 3.2

$$
\begin{align}
\max_\pi \mathcal H(\pi)-\psi^*(\rho_\pi-\rho_E)
\end{align}
$$

[Ho et al. 2016](#ref1) propose the following conjugate

$$
\begin{align}
\psi^*_{GA}(\rho_\pi-\rho_E)=\max_{D}\mathbb E_\pi[\log(1-D(s,a))]+\mathbb E_{\pi_E}[\log(D(s,a))]
\end{align}
$$

which is exactly the discriminator objective in GAN. As in GANs, we can show that the optimal discriminator is \\(D_*(s,a)={\pi_E(s,a)\over \pi_E(s,a)+\pi(s,a)}\\) by taking the derivative of the above objective and setting to zero. Furthermore, this optimal loss corresponds to the Jensen-Shannon divergence between policies

$$
\begin{align}
\psi^*_{GA}(\rho_\pi-\rho_E)=&\mathbb E_\pi\left[\log{\pi(s,a)\over\pi(s,a)+\pi_E(s,a)}\right]+\mathbb E_{\pi_E}\left[\log{\pi_E\over\pi(s,a)+\pi_E(s,a)}\right]\\\
=&\mathbb E_\pi\left[\log{\pi(s,a)\over{1\over 2}(\pi(s,a)+\pi_E(s,a))}\right]+\mathbb E_{\pi_E}\left[\log{\pi_E\over{1\over 2}(\pi(s,a)+\pi_E(s,a))}\right]-2\log 2\\\
=&D_{TV}(\pi\Vert\pi_E)
\end{align}
$$


### Losses

We discuss the following 

## Experimental Results

<figure>
  <img src="{{ '/images/IL/GAIL-Figure1.png' | absolute_url }}" alt="" width="1000">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

Figure 1 shows that GAIL performs extremely well across all tasks; it basically replicates the behavior of the expert data in all environments. Moreover, GAIL only requires a small amount of expert trajectories.

## References

<a name="ref1"></a>Ho, Jonathan, and Stefano Ermon. 2016. “Generative Adversarial Imitation Learning.” *Advances in Neural Information Processing Systems*, 4572–80.

## Supplementary Materials

### Proofs

<a name="proposition3.2"></a>**Proposition 3.2** \\(\text{RL}\circ\text{IRL}(\pi_E)=\arg\max_{\pi}\mathcal H(\pi)-\psi^*(\rho_\pi-\rho_E)\\).

Let \\(\tilde r\in\text{IRL}(\pi_E)\\) and \\(\tilde \pi\in\text{RL}(\tilde r)=\text{RL}\circ\text{IRL}(\pi_E)\\), and

$$
\begin{align}
\pi_A\in&\arg\max_\pi\mathcal H(\pi)-\psi^*(\rho_\pi-\rho_{\pi_E})\\\
=&\arg\max_\pi\min_r\mathcal H(\pi)+\psi(r)-\sum_{s,a}(\rho_\pi(s,a)-\rho_{\pi_E}(s,a))r(s,a)
\end{align}
$$

Let \\(\rho_A\\) be the occupancy measure of \\(\pi_A\\), \\(\tilde \rho\\) be the occupancy measure of \\(\tilde \pi\\), and define the Lagrangian

$$
\begin{align}
\bar{\mathcal L}(\rho,r)=\bar {\mathcal H}(\rho)+\psi(r)-\sum_{s,a}(\rho(s,a)-\rho_{\pi_E}(s,a))r(s,a)
\end{align}
$$

Because there is a one-to-one correspondence between a policy and its occupancy measure, we have

$$
\begin{align}
\rho_A=\arg\max_\pi\min_r\bar{\mathcal L}(\rho,r)
\end{align}
$$

By Equation \\(\eqref{eq:2}\\), we have

$$
\begin{align}
\tilde r=&\arg\min_r\max_\rho\bar{\mathcal L}(\rho,r)\\\
\tilde \rho=&\arg\max_\rho\bar{\mathcal L}(\rho,\tilde r)
\end{align}
$$

Now we show \\(\rho_A=\tilde \rho\\). Because \\(\mathcal H\\) is concave and \\(\psi\\) is convex, we have that \\(\bar{\mathcal L}(\cdot,r)\\) is concave and \\(\bar{\mathcal L}(\rho,\cdot)\\) is convex. Therefore, we use minimax duality

$$
\begin{align}
\min_r\max_\rho\bar{\mathcal L}(\rho,r)=\max_\rho\min_r\bar{\mathcal L}(\rho,r)
\end{align}
$$

This implies \\(\rho_A=\tilde \rho\\) and thus \\(\pi_A=\tilde \pi\\).